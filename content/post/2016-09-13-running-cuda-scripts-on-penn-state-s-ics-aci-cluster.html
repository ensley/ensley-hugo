---
title: Running CUDA Scripts on Penn State's ICS-ACI Cluster
date: '2016-09-13'
categories: ["Research"]
tags: ["C", "Cluster", "CUDA", "R", "Rstats", "GPU", "Research"]
---

<!-- BLOGDOWN-HEAD -->
<!-- /BLOGDOWN-HEAD -->

<!-- BLOGDOWN-BODY-BEFORE -->
<!-- /BLOGDOWN-BODY-BEFORE -->
<p>This is for my own reference, because if I go more than a few days without doing these steps I will probably forget them.</p>
<ol style="list-style-type: decimal">
<li>If there are files on your local computer that you need to use on the cluster, transfer them over with this command.</li>
</ol>
<pre class="bash"><code>scp path/to/file/file1.c other/path/file2.c abc123@lionga.rcc.psu.edu:~/work</code></pre>
<p>To copy an entire directory over:</p>
<pre class="bash"><code>scp -r path/to/directory abc123@lionga.rcc.psu.edu:~/work</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>The Lion-GA cluster is the one with the Nvidia graphics cards. SSH into it with -Y so that you can launch a text editor with a GUI. I refuse to learn vim or emacs.</li>
</ol>
<pre class="bash"><code>ssh -Y abc123@lionga.rcc.psu.edu</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Once you’re in Lion-GA, to see all available GPU nodes, do <code>pbsnodes</code>. There are 8 total as of right now. It seems like there are always a few that are offline. To see which are currently offline, do <code>pbsnodes -l</code>.</li>
<li>SSH into one of the nodes that are online.</li>
</ol>
<pre class="bash"><code>ssh lionga1</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>To see a list of the 8 graphics cards and any processes that are currently running on them, do <code>nvidia-smi</code>.</li>
<li>Once you are SSH’d into the GPU node, you can compile CUDA .cu files using <code>nvcc</code> and run the output, just as you would on your local machine.</li>
<li>To edit a file, logout of the GPU node and then run</li>
</ol>
<pre class="bash"><code>gedit file.c</code></pre>
