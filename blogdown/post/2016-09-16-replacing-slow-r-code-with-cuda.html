---
title: Replacing Slow R Code with CUDA
date: '2016-09-16'
categories: ["Research"]
tags: ["C", "R", "Rstats", "CUDA", "Research", "GPU"]
---

<!-- BLOGDOWN-HEAD -->



<!-- /BLOGDOWN-HEAD -->

<!-- BLOGDOWN-BODY-BEFORE -->

<!-- /BLOGDOWN-BODY-BEFORE -->

<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>Suppose we have a vector <span class="math inline">\(\{x_i: i = 1, \dots, N\}\)</span> and a collection of <span class="math inline">\(\{s_j: j = 1, \dots, M\}\)</span> random samples. For each <span class="math inline">\(x_j\)</span>, we want to calculate <span class="math display">\[
\frac{1}{M} \sum_{j=1}^M \cos(x_is_j).
\]</span></p>
</div>
<div id="naive-r-solution" class="section level2">
<h2>Naive R Solution</h2>
<p>If you were new to R, you might do something like this:</p>
<pre class="r"><code>func1 &lt;- function(x, s) {
  N &lt;- length(x)
  M &lt;- length(s)
  y &lt;- rep(0, N)
   
  for(i in 1:N) {
    for(j in 1:M) {
      y[i] &lt;- y[i] + cos(x[i] * s[j])
    }
    y[i] &lt;- y[i] / M
  }
   
  return(y)
}</code></pre>
<p>If you’ve had any experience with R, you probably recoiled in horror at the sight of those nested for loops. For loops are notoriously slow in R, and the function above is doing <span class="math inline">\(N \times M\)</span> iterations with them. In fact, that’s probably the single worst way I could have written that function. If <span class="math inline">\(N\)</span> and <span class="math inline">\(M\)</span> are small then it doesn’t matter much, but even if <span class="math inline">\(N=M=1000\)</span> (not big at all for most practical applications), look what happens to the runtime:</p>
<pre><code>Unit: seconds
        expr      min       lq     mean  median      uq      max neval
 func1(x, s) 1.608869 1.650864 1.718337 1.67365 1.77996 2.070576   100</code></pre>
<p>It takes about 1.7 seconds on average to run. This is not ideal, especially in my research, where I need <span class="math inline">\(N=80000\)</span> and <span class="math inline">\(M=10000\)</span> at least, and then I need it to run tens of thousands of times.</p>
</div>
<div id="a-faster-r-solution" class="section level2">
<h2>A Faster R Solution</h2>
<p>Let’s try a different approach. R has <em>vectorized</em> functions, like <code>mean(x)</code>, which are much, much faster than doing the equivalent calculation with a loop.</p>
<pre class="r"><code>func2 &lt;- Vectorize( function(x, s) {
  mean(cos(x*s))
}, vectorize.args = &#39;x&#39;)</code></pre>
<p>The calculation is taken care of in one line, <code>mean(cos(x*s))</code>. The function is wrapped in <code>Vectorize</code>, which tells R to apply the function to every element of <code>x</code>, one at a time. If we run this one with <span class="math inline">\(N=M=1000\)</span>, we get</p>
<pre><code>Unit: milliseconds
        expr      min       lq     mean   median       uq      max neval
 func2(x, s) 27.50761 29.18643 31.93724 31.30861 33.46475 50.32115   100</code></pre>
<p>That’s more like it. This runs in about 30 milliseconds, 50 times faster than the first function. Let’s bump up the sizes to <span class="math inline">\(N=M=10000\)</span> and see what happens.</p>
<pre><code>Unit: seconds
        expr      min       lq     mean   median       uq      max neval
 func2(x, s) 1.956879 2.039474 2.186676 2.151696 2.296424 3.011084   100</code></pre>
<p>Hmm. The evaluation time has climbed up to more than 2 seconds. It doesn’t sound like much, but it really adds up when you’re calling this function thousands of times.</p>
</div>
<div id="parallelization-with-cuda" class="section level2">
<h2>Parallelization with CUDA</h2>
<p>We could rewrite the function in C, a lower-level language than R, but this only speeds things up by a factor of 2 or 3 at best. The main reason why this is so slow is that R (and ordinary C as well) is <em>single-threaded</em>. This means that R can only do one thing at a time. It takes <span class="math inline">\(x_1\)</span>, calculates <span class="math inline">\(\frac{1}{M}\sum^M_{j=1}\cos(x_1s_j)\)</span>, then takes <span class="math inline">\(x_2\)</span>, calculates <span class="math inline">\(\frac{1}{M}\sum^M_{j=1}\cos(x_2s_j)\)</span>, and so on until it hits <span class="math inline">\(x_N\)</span>. But there’s no reason that things <em>must</em> be evaluated so sequentially. It would save a lot of time if we could do that calculation for lots of different <span class="math inline">\(x_i\)</span>’s simultaneously. This is called <span class="math inline">\(parallelization\)</span>.</p>
<p>CUDA is an API developed by Nvidia for doing parallel computing on a GPU (graphics processing unit). GPUs are designed with parallel computing in mind. They are essentially made up of thousands of little processors that are organized in such a way that they can split up a task, execute part of it, and recombine the results. Here is a CUDA file that accomplishes our task. I named it <code>kernel.cu</code>.</p>
<pre class="c"><code>#include &lt;stdio.h&gt;
#include &lt;math.h&gt;
#include &lt;R.h&gt;
#define TPB 256
 
__device__
double mcCalc(double x, double *d_samps, int S)
{
    double total = 0.0f;
    for(int i = 0; i &lt; S; ++i)
    {
        total += cos(x * d_samps[i]);
    }
    return total / S;
}
 
__global__
void mcKernel(double *d_vec, double *d_samps, double *d_mat, int N, int S)
{
    const int i = blockIdx.x*blockDim.x + threadIdx.x;
    if(i &gt;= N) return;
    const double x = d_vec[i];
    d_mat[i] = mcCalc(x, d_samps, S);
}
 
extern &quot;C&quot; void mc(double *vec, double *samps, double *mat, int *Np, int *Sp)
{
    cudaEvent_t startMemcpy, stopMemcpy;
    cudaEvent_t startKernel, stopKernel;
    cudaEventCreate(&amp;startMemcpy);
    cudaEventCreate(&amp;stopMemcpy);
    cudaEventCreate(&amp;startKernel);
    cudaEventCreate(&amp;stopKernel);
     
    double *d_vec = 0;
    double *d_samps = 0;
    double *d_mat = 0;
    int N = *Np;
    int S = *Sp;
     
    cudaMalloc(&amp;d_vec, N*sizeof(double));
    cudaMalloc(&amp;d_samps, S*sizeof(double));
    cudaMalloc(&amp;d_mat, N*sizeof(double));
     
    // Record the event that &quot;starts the clock&quot; on data transfer.
    cudaEventRecord(startMemcpy);
    cudaMemcpy(d_vec, vec, N*sizeof(double), cudaMemcpyHostToDevice);
    cudaMemcpy(d_samps, samps, S*sizeof(double), cudaMemcpyHostToDevice);
    // Record the event that &quot;stops the clock&quot; on data transfer.
    cudaEventRecord(stopMemcpy);
     
    // Record the event that &quot;starts the clock&quot; on kernel execution.
    cudaEventRecord(startKernel);
    mcKernel&lt;&lt;&lt;(N+TPB-1)/TPB, TPB&gt;&gt;&gt;(d_vec, d_samps, d_mat, N, S);
    // Record the event that &quot;stops the clock&quot; on kernel execution.
    cudaEventRecord(stopKernel);
     
    // Copy result from device to host.
    cudaMemcpy(mat, d_mat, N*sizeof(double), cudaMemcpyDeviceToHost);
     
    // Ensure timed events have stopped.
    cudaEventSynchronize(stopMemcpy);
    cudaEventSynchronize(stopKernel);
     
    // Convert event records to time and output.
    float memcpyTimeInMs = 0;
    cudaEventElapsedTime(&amp;memcpyTimeInMs, startMemcpy, stopMemcpy);
    float kernelTimeInMs = 0;
    cudaEventElapsedTime(&amp;kernelTimeInMs, startKernel, stopKernel);
    printf(&quot;Kernel time (ms): %f\n&quot;, kernelTimeInMs);
    printf(&quot;Data transfer time (ms): %f\n&quot;, memcpyTimeInMs);
     
    cudaFree(d_vec);
    cudaFree(d_samps);
    cudaFree(d_mat);
}</code></pre>
</div>
<div id="using-cuda-in-r" class="section level2">
<h2>Using CUDA in R</h2>
<p>To use this in R, we need to compile it as a shared library object.</p>
<pre class="bash"><code>nvcc -O3 -I/usr/global/R/3.0.1/include -G --shared -Xcompiler -fPIC -o main.so kernel.cu</code></pre>
<p>Now, in an R session, first we need to load that compiled library with <code>dyn.load()</code>, and then we can write a wrapper function around the call to C.</p>
<pre class="r"><code>func3 &lt;- function(x, s) {
    tmp &lt;- .C(&#39;mc&#39;, as.double(x), as.double(s), res = as.double(rep(0.0, length(x))), as.integer(length(x)), as.integer(length(s)))
    tmp$res
}</code></pre>
<p>Finally, we can use this function just like any other regular R function. When I tried this with <span class="math inline">\(N=80200\)</span> and <span class="math inline">\(M=10000\)</span>, it ran in about a third of a second. Pretty amazing if you ask me.</p>
<p><strong>NOTE:</strong> If you don’t have an Nvidia GPU in your computer, this will not work for you. I don’t have one in mine, so I needed to use <a href="https://ics.psu.edu/advanced-cyberinfrastructure/">Penn State’s ICS-ACI computing cluster</a>, which was an adventure in itself. I have written <a href="/post/2016-09-13-running-cuda-scripts-on-penn-state-s-ics-aci-cluster/">another blog post</a> about how to do that.</p>
</div>
